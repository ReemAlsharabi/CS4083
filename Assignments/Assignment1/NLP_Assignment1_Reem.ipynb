{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO6X53OKjpZnqTo18gWI/rX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ReemAlsharabi/CS4083/blob/main/Assignments/Assignment1/NLP_Assignment1_Reem.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Name: Reem Alsharabi\n",
        "\n",
        "ID: S20106353"
      ],
      "metadata": {
        "id": "7U2t3upRP35E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "About the data: I'm using Gutenberg corpus, which provides a collection of freely available books.\n",
        "\n",
        "The book I chose is Lewis Carroll's \"Alice's Adventures in Wonderland\""
      ],
      "metadata": {
        "id": "ozQIEoeWQMFX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nl8gpI_cP0Hx",
        "outputId": "6a654af2-d4aa-48eb-8664-b7f53e008ac1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.corpus import gutenberg\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tag import pos_tag"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "book_id = 'carroll-alice.txt'\n",
        "book_text = gutenberg.raw(book_id)\n",
        "\n",
        "# Tokenize the book into words\n",
        "words = word_tokenize(book_text)\n",
        "\n",
        "# Part-of-speech (POS) tagging\n",
        "pos_tags = pos_tag(words)"
      ],
      "metadata": {
        "id": "rswq4ujBP-mc"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ch1 tasks\n"
      ],
      "metadata": {
        "id": "mPJSlqIcRW6A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Count WH words\n"
      ],
      "metadata": {
        "id": "h5HsWyVFSF-j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wh_words = [word for word, tag in pos_tags if tag.startswith('W')]\n",
        "wh_word_count = len(wh_words)\n",
        "print(\"Count of WH words:\", wh_word_count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i11FmodWRHJx",
        "outputId": "797bb9ad-6b09-43bd-c663-b8c225e749c5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Count of WH words: 448\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explore the frequency of each word in the book"
      ],
      "metadata": {
        "id": "N5PqenumSO4X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove stopwords\n",
        "stopwords = set(stopwords.words('english'))\n",
        "filtered_words = [word for word in words if word.lower() not in stopwords]\n",
        "\n",
        "# Calculate the frequency distribution of words\n",
        "fdist = FreqDist(filtered_words)\n",
        "\n",
        "# Print a limited number of words\n",
        "num_words = 10  # Number of words to display\n",
        "\n",
        "# Print the frequency of each word\n",
        "for i, (word, frequency) in enumerate(fdist.items()):\n",
        "    print(word, \":\", frequency)\n",
        "    if i >= num_words - 1:\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZaTfVxcsSN1P",
        "outputId": "20a3ef9a-928d-48c9-96da-0e7a4849385f"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ : 3\n",
            "Alice : 394\n",
            "'s : 188\n",
            "Adventures : 3\n",
            "Wonderland : 3\n",
            "Lewis : 1\n",
            "Carroll : 1\n",
            "1865 : 1\n",
            "] : 3\n",
            "CHAPTER : 12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explore WordNet senses for an ambiguous word"
      ],
      "metadata": {
        "id": "CRkhFloKSw_h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get synsets for the word \"tree\"\n",
        "synsets = wordnet.synsets('tree')\n",
        "\n",
        "# Print the definitions of each synset\n",
        "for synset in synsets:\n",
        "    print(\"Synset:\", synset.name())\n",
        "    print(\"Definition:\", synset.definition())\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pnznog3mS9eB",
        "outputId": "7f266230-679c-4981-d8aa-34557530f6a7"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Synset: tree.n.01\n",
            "Definition: a tall perennial woody plant having a main trunk and branches forming a distinct elevated crown; includes both gymnosperms and angiosperms\n",
            "\n",
            "Synset: tree.n.02\n",
            "Definition: a figure that branches from a single root\n",
            "\n",
            "Synset: tree.n.03\n",
            "Definition: English actor and theatrical producer noted for his lavish productions of Shakespeare (1853-1917)\n",
            "\n",
            "Synset: corner.v.02\n",
            "Definition: force a person or an animal into a position from which he cannot escape\n",
            "\n",
            "Synset: tree.v.02\n",
            "Definition: plant with trees\n",
            "\n",
            "Synset: tree.v.03\n",
            "Definition: chase an animal up a tree\n",
            "\n",
            "Synset: tree.v.04\n",
            "Definition: stretch (a shoe) on a shoetree\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explore hyponyms and hypernyms of two synsets:"
      ],
      "metadata": {
        "id": "jt-vmd3mTG1K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get synsets for the words \"sky\" and \"animal\"\n",
        "sky_synsets = wordnet.synsets('sky')\n",
        "animal_synsets = wordnet.synsets('animal')\n",
        "\n",
        "# Print the hyponyms and hypernyms of each synset\n",
        "for synset in sky_synsets:\n",
        "    print(\"Synset:\", synset.name())\n",
        "    print(\"Hyponyms:\", synset.hyponyms())\n",
        "    print(\"Hypernyms:\", synset.hypernyms())\n",
        "    print()\n",
        "\n",
        "for synset in animal_synsets:\n",
        "    print(\"Synset:\", synset.name())\n",
        "    print(\"Hyponyms:\", synset.hyponyms())\n",
        "    print(\"Hypernyms:\", synset.hypernyms())\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtAnnAr_S5lf",
        "outputId": "4b3ed754-467e-4200-a0c1-55c41878af89"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Synset: sky.n.01\n",
            "Hyponyms: [Synset('blue_sky.n.01'), Synset('mackerel_sky.n.01')]\n",
            "Hypernyms: [Synset('atmosphere.n.05')]\n",
            "\n",
            "Synset: flip.v.06\n",
            "Hyponyms: [Synset('lag.v.03'), Synset('submarine.v.02'), Synset('throw_back.v.01')]\n",
            "Hypernyms: [Synset('fling.v.01')]\n",
            "\n",
            "Synset: animal.n.01\n",
            "Hyponyms: [Synset('acrodont.n.01'), Synset('adult.n.02'), Synset('biped.n.01'), Synset('captive.n.02'), Synset('chordate.n.01'), Synset('creepy-crawly.n.01'), Synset('critter.n.01'), Synset('darter.n.02'), Synset('domestic_animal.n.01'), Synset('embryo.n.02'), Synset('feeder.n.06'), Synset('female.n.01'), Synset('fictional_animal.n.01'), Synset('game.n.04'), Synset('giant.n.01'), Synset('herbivore.n.01'), Synset('hexapod.n.01'), Synset('homeotherm.n.01'), Synset('insectivore.n.02'), Synset('invertebrate.n.01'), Synset('larva.n.01'), Synset('male.n.01'), Synset('marine_animal.n.01'), Synset('mate.n.03'), Synset('metazoan.n.01'), Synset('migrator.n.02'), Synset('molter.n.01'), Synset('mutant.n.02'), Synset('omnivore.n.02'), Synset('peeper.n.03'), Synset('pest.n.04'), Synset('pet.n.01'), Synset('pleurodont.n.01'), Synset('poikilotherm.n.01'), Synset('predator.n.02'), Synset('prey.n.02'), Synset('racer.n.03'), Synset('range_animal.n.01'), Synset('scavenger.n.03'), Synset('stayer.n.01'), Synset('stunt.n.02'), Synset('survivor.n.03'), Synset('thoroughbred.n.03'), Synset('varmint.n.02'), Synset('work_animal.n.01'), Synset('young.n.01'), Synset('zooplankton.n.01')]\n",
            "Hypernyms: [Synset('organism.n.01')]\n",
            "\n",
            "Synset: animal.s.01\n",
            "Hyponyms: []\n",
            "Hypernyms: []\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compute the average polysemy of nouns, verbs, adjectives, and adverbs:"
      ],
      "metadata": {
        "id": "ACENL0MeTZMp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the part-of-speech tags for the four word types\n",
        "noun_tags = ['NN', 'NNS', 'NNP', 'NNPS']\n",
        "verb_tags = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
        "adj_tags = ['JJ', 'JJR', 'JJS']\n",
        "adv_tags = ['RB', 'RBR', 'RBS']\n",
        "\n",
        "# Group words by their part-of-speech tags\n",
        "nouns = [word for word, tag in pos_tags if tag in noun_tags]\n",
        "verbs = [word for word, tag in pos_tags if tag in verb_tags]\n",
        "adjs = [word for word, tag in pos_tags if tag in adj_tags]\n",
        "advs = [word for word, tag in pos_tags if tag in adv_tags]\n",
        "\n",
        "# Function to calculate average polysemy\n",
        "def calculate_avg_polysemy(words):\n",
        "    polysemy_counts = [len(wordnet.synsets(word)) for word in words]\n",
        "    return sum(polysemy_counts) / len(words)\n",
        "\n",
        "# Calculate average polysemy for each word type\n",
        "noun_avg_polysemy = calculate_avg_polysemy(nouns)\n",
        "verb_avg_polysemy = calculate_avg_polysemy(verbs)\n",
        "adj_avg_polysemy = calculate_avg_polysemy(adjs)\n",
        "adv_avg_polysemy = calculate_avg_polysemy(advs)\n",
        "\n",
        "print(f\"Average Polysemy: \\nNouns: {noun_avg_polysemy} \\nVerbs: {verb_avg_polysemy} \\nAdjectives: {adj_avg_polysemy} \\nAdverbs: {adv_avg_polysemy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rm8pFLrrTNH7",
        "outputId": "b2c9711e-f6f1-4e1a-df1b-748778ed0718"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Polysemy: \n",
            "Nouns: 7.1527987897125564 \n",
            "Verbs: 14.126315789473685 \n",
            "Adjectives: 9.004563233376793 \n",
            "Adverbs: 5.749395258829221\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y1ycfHoUTeDV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}